"use strict";
/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at https://mozilla.org/MPL/2.0/. */
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.PeerNetwork = void 0;
const rust_nodejs_1 = require("@ironfish/rust-nodejs");
const blru_1 = __importDefault(require("blru"));
const buffer_map_1 = require("buffer-map");
const assert_1 = require("../assert");
const consensus_1 = require("../consensus");
const event_1 = require("../event");
const config_1 = require("../fileStores/config");
const logger_1 = require("../logger");
const metrics_1 = require("../metrics");
const package_1 = require("../package");
const platform_1 = require("../platform");
const primitives_1 = require("../primitives");
const block_1 = require("../primitives/block");
const utils_1 = require("../utils");
const blockFetcher_1 = require("./blockFetcher");
const cannotSatisfyRequest_1 = require("./messages/cannotSatisfyRequest");
const disconnecting_1 = require("./messages/disconnecting");
const getBlockHeaders_1 = require("./messages/getBlockHeaders");
const getBlocks_1 = require("./messages/getBlocks");
const getBlockTransactions_1 = require("./messages/getBlockTransactions");
const getCompactBlock_1 = require("./messages/getCompactBlock");
const networkMessage_1 = require("./messages/networkMessage");
const newBlockHashes_1 = require("./messages/newBlockHashes");
const newCompactBlock_1 = require("./messages/newCompactBlock");
const newPooledTransactionHashes_1 = require("./messages/newPooledTransactionHashes");
const newTransactions_1 = require("./messages/newTransactions");
const pooledTransactions_1 = require("./messages/pooledTransactions");
const rpcNetworkMessage_1 = require("./messages/rpcNetworkMessage");
const connections_1 = require("./peers/connections");
const localPeer_1 = require("./peers/localPeer");
const peer_1 = require("./peers/peer");
const peerConnectionManager_1 = require("./peers/peerConnectionManager");
const peerManager_1 = require("./peers/peerManager");
const transactionFetcher_1 = require("./transactionFetcher");
const parseUrl_1 = require("./utils/parseUrl");
const serializers_1 = require("./utils/serializers");
const version_1 = require("./version");
const webSocketServer_1 = require("./webSocketServer");
/**
 * We store gossips that have already been seen and processed, and ignore them
 * if we have seen them before. The set that contains these gossips is
 * bounded to a specific size and old ones are evicted in the order
 * they were inserted.
 */
const GOSSIP_FILTER_SIZE = 100000;
const GOSSIP_FILTER_FP_RATE = 0.000001;
const MAX_GET_BLOCK_TRANSACTIONS_DEPTH = 10;
const MAX_GET_COMPACT_BLOCK_DEPTH = 5;
/**
 * Entry point for the peer-to-peer network. Manages connections to other peers on the network
 * and provides abstractions for several methods of sending/receiving network messages.
 */
class PeerNetwork {
    constructor(options) {
        this.onIsReadyChanged = new event_1.Event();
        this.onTransactionAccepted = new event_1.Event();
        this.onBlockGossipReceived = new event_1.Event();
        this.onTransactionGossipReceived = new event_1.Event();
        this.started = false;
        // A cache that keeps track of transactions that are a part of recently confirmed blocks
        // This is useful for filtering out downloading of transaction hashes that were recently added
        this.recentlyAddedToChain = new blru_1.default(300 * 60, null, buffer_map_1.BufferMap);
        // A cache that keeps track of which peers have seen which transactions. This allows
        // us to not send the same transaction to a peer more than once. TODO(daniel): We want to
        // change this to use an LRU cache so that we don't get false positives
        this.knownTransactionFilter = new rust_nodejs_1.RollingFilter(GOSSIP_FILTER_SIZE * 50, GOSSIP_FILTER_FP_RATE);
        /**
         * If the peer network is ready for messages to be sent or not
         */
        this._isReady = false;
        this.networkId = options.networkId;
        this.enableSyncing = options.enableSyncing ?? true;
        this.node = options.node;
        this.chain = options.chain;
        this.logger = (options.logger || logger_1.createRootLogger()).withTag('peernetwork');
        this.metrics = options.metrics || new metrics_1.MetricsMonitor({ logger: this.logger });
        this.telemetry = options.telemetry;
        this.bootstrapNodes = options.bootstrapNodes || [];
        this.incomingWebSocketWhitelist = options.incomingWebSocketWhitelist || [];
        this.localPeer = new localPeer_1.LocalPeer(options.identity, options.agent || platform_1.Platform.getAgent(package_1.IronfishPKG), version_1.VERSION_PROTOCOL, options.chain, options.webSocket, options.networkId, this.enableSyncing);
        this.localPeer.port = options.port === undefined ? null : options.port;
        this.localPeer.name = options.name || null;
        this.localPeer.simulateLatency = options.simulateLatency || 0;
        const maxPeers = options.maxPeers || 10000;
        const targetPeers = options.targetPeers || 50;
        const logPeerMessages = options.logPeerMessages ?? false;
        this.peerManager = new peerManager_1.PeerManager(this.localPeer, options.hostsStore, this.logger, this.metrics, maxPeers, targetPeers, logPeerMessages, options.stunServers);
        this.peerManager.onMessage.on((peer, message) => this.handleMessage(peer, message));
        this.peerManager.onConnectedPeersChanged.on(() => {
            this.metrics.p2p_PeersCount.value = this.peerManager.getConnectedPeers().length;
            this.updateIsReady();
        });
        this.peerConnectionManager = new peerConnectionManager_1.PeerConnectionManager(this.peerManager, this.logger, {
            maxPeers,
        });
        this.minPeers = options.minPeers || 1;
        this.listen = options.listen === undefined ? true : options.listen;
        this.requests = new Map();
        if (options.name && options.name.length > 32) {
            options.name = options.name.slice(32);
        }
        this.blockFetcher = new blockFetcher_1.BlockFetcher(this);
        this.transactionFetcher = new transactionFetcher_1.TransactionFetcher(this);
        this.chain.onConnectBlock.on((block) => {
            this.blockFetcher.removeBlock(block.header.hash);
            for (const transaction of block.transactions) {
                this.transactionFetcher.removeTransaction(transaction.hash());
                this.recentlyAddedToChain.set(transaction.hash(), true);
            }
        });
        this.chain.onForkBlock.on((block) => {
            this.blockFetcher.removeBlock(block.header.hash);
        });
        this.chain.onDisconnectBlock.on((block) => {
            for (const transaction of block.transactions) {
                this.recentlyAddedToChain.remove(transaction.hash());
            }
        });
        this.node.miningManager.onNewBlock.on((block) => {
            this.broadcastBlock(block);
            this.broadcastBlockHash(block.header);
        });
    }
    get isReady() {
        return this._isReady;
    }
    start() {
        if (this.started) {
            return;
        }
        this.started = true;
        // Start the WebSocket server if possible
        if (this.listen && 'Server' in this.localPeer.webSocket && this.localPeer.port !== null) {
            this.webSocketServer = new webSocketServer_1.WebSocketServer(this.localPeer.webSocket.Server, this.localPeer.port);
            this.webSocketServer.onStart(() => {
                const address = this.webSocketServer?.server.address();
                const addressStr = typeof address === 'object' ? `${address.address}:${address.port}` : String(address);
                this.logger.info(`WebSocket server started at ${addressStr}`);
            });
            this.webSocketServer.onConnection((connection, req) => {
                let address = null;
                if (req.headers['X-Forwarded-For'] && req.headers['X-Forwarded-For'][0]) {
                    address = req.headers['X-Forwarded-For'][0];
                }
                else if (req.socket.remoteAddress) {
                    address = req.socket.remoteAddress;
                }
                if (address) {
                    // Some times local peers connect on IPV6 incompatible addresses like
                    // '::ffff:127.0.0.1' and we don't support connecting over IPv6 right now
                    address = address.replace('::ffff:', '');
                }
                const isWhitelisted = address && this.incomingWebSocketWhitelist.includes(address);
                if (this.peerManager.shouldRejectDisconnectedPeers() && !isWhitelisted) {
                    this.logger.debug('Disconnecting inbound websocket connection because the node has max peers');
                    const disconnect = new disconnecting_1.DisconnectingMessage({
                        destinationIdentity: null,
                        disconnectUntil: this.peerManager.getCongestedDisconnectUntilTimestamp(),
                        reason: disconnecting_1.DisconnectingReason.Congested,
                        sourceIdentity: this.localPeer.publicIdentity,
                    });
                    connection.send(disconnect.serialize());
                    connection.close();
                    return;
                }
                this.peerManager.createPeerFromInboundWebSocketConnection(connection, address);
            });
            this.peerManager.onConnect.on((peer) => {
                this.logger.debug(`Connected to ${peer.getIdentityOrThrow()}`);
            });
            this.peerManager.onDisconnect.on((peer) => {
                this.logger.debug(`Disconnected from ${String(peer.state.identity)}`);
            });
            this.onIsReadyChanged.on((isReady) => {
                if (isReady) {
                    this.logger.info(`Connected to the Iron Fish network`);
                    this.node.onPeerNetworkReady();
                }
                else {
                    this.logger.info(`Not connected to the Iron Fish network`);
                    this.node.onPeerNetworkNotReady();
                }
            });
        }
        // Start up the PeerManager
        this.peerManager.start();
        // Start up the PeerConnectionManager
        this.peerConnectionManager.start();
        this.updateIsReady();
        for (const node of this.bootstrapNodes) {
            const url = parseUrl_1.parseUrl(node);
            if (!url.hostname) {
                throw new Error(`Could not determine a hostname for bootstrap node "${node}". Is it formatted correctly?`);
            }
            // If the user has not specified a port, we can guess that
            // it's running on the default ironfish websocket port
            const port = url.port ? url.port : config_1.DEFAULT_WEBSOCKET_PORT;
            this.peerManager.connectToWebSocketAddress({
                host: url.hostname,
                port,
                whitelist: true,
            });
        }
    }
    /**
     * Call close when shutting down the PeerNetwork to clean up
     * outstanding connections.
     */
    async stop() {
        this.started = false;
        this.peerConnectionManager.stop();
        await this.peerManager.stop();
        this.webSocketServer?.close();
        this.updateIsReady();
        this.blockFetcher.stop();
        this.transactionFetcher.stop();
    }
    /**
     * Send a compact block to a sqrt subset of peers who haven't yet received the block
     */
    broadcastBlock(block) {
        const hash = block.header.hash;
        const peersToSendToArray = utils_1.ArrayUtils.shuffle([...this.connectedPeersWithoutBlock(hash)]);
        const sqrtSize = Math.floor(Math.sqrt(peersToSendToArray.length));
        const compactBlockMessage = new newCompactBlock_1.NewCompactBlockMessage(block.toCompactBlock());
        // Send compact block to random subset of sqrt of peers
        for (const peer of peersToSendToArray.slice(0, sqrtSize)) {
            if (peer.send(compactBlockMessage)) {
                peer.knownBlockHashes.set(hash, peer_1.KnownBlockHashesValue.Sent);
            }
        }
    }
    /**
     * Send a block hash to all connected peers who haven't yet received the block.
     */
    broadcastBlockHash(header) {
        const hashMessage = new newBlockHashes_1.NewBlockHashesMessage([
            { hash: header.hash, sequence: header.sequence },
        ]);
        for (const peer of this.connectedPeersWithoutBlock(header.hash)) {
            if (peer.send(hashMessage)) {
                peer.knownBlockHashes.set(header.hash, peer_1.KnownBlockHashesValue.Sent);
            }
        }
    }
    /**
     * Send out the transaction only to peers that have not yet heard about it.
     * The full transaction will be sent to a subset of sqrt(num_peers)
     * and the rest of the peers will receive the transaction hash
     */
    broadcastTransaction(transaction) {
        const hash = transaction.hash();
        const peersToSendToArray = utils_1.ArrayUtils.shuffle([
            ...this.connectedPeersWithoutTransaction(hash),
        ]);
        const sqrtSize = Math.floor(Math.sqrt(peersToSendToArray.length));
        const fullTransactionMessage = new newTransactions_1.NewTransactionsMessage([transaction]);
        const hashMessage = new newPooledTransactionHashes_1.NewPooledTransactionHashes([hash]);
        // Send full transaction to random subset of sqrt of peers
        for (const peer of peersToSendToArray.slice(0, sqrtSize)) {
            if (peer.state.type !== 'CONNECTED') {
                continue;
            }
            if (peer.send(fullTransactionMessage)) {
                this.markKnowsTransaction(hash, peer.state.identity);
            }
        }
        // Send just the hash to the remaining peers
        for (const peer of peersToSendToArray.slice(sqrtSize)) {
            if (peer.state.type !== 'CONNECTED') {
                continue;
            }
            if (peer.send(hashMessage)) {
                this.markKnowsTransaction(hash, peer.state.identity);
            }
        }
    }
    knowsTransaction(hash, peerId) {
        const toTest = Buffer.concat([hash, Buffer.from(peerId)]);
        return this.knownTransactionFilter.test(toTest);
    }
    markKnowsTransaction(hash, peerId) {
        const toAdd = Buffer.concat([hash, Buffer.from(peerId)]);
        this.knownTransactionFilter.add(toAdd);
    }
    *connectedPeersWithoutTransaction(hash) {
        for (const p of this.peerManager.identifiedPeers.values()) {
            if (p.state.type === 'CONNECTED' &&
                p.features?.syncing &&
                !this.knowsTransaction(hash, p.state.identity)) {
                yield p;
            }
        }
    }
    *connectedPeersWithoutBlock(hash) {
        for (const p of this.peerManager.identifiedPeers.values()) {
            if (p.state.type === 'CONNECTED' &&
                p.features?.syncing &&
                !p.knownBlockHashes.has(hash)) {
                yield p;
            }
        }
    }
    /**
     * Fire an RPC request to the given peer identity. Returns a promise that
     * will resolve when the response is received, or will be rejected if the
     * request cannot be completed before timing out.
     */
    requestFrom(peer, message) {
        const rpcId = message.rpcId;
        return new Promise((resolve, reject) => {
            // Reject requests if the connection becomes disconnected
            const onConnectionStateChanged = () => {
                const request = this.requests.get(rpcId);
                if (request && request.peer.state.type === 'DISCONNECTED') {
                    request.peer.onStateChanged.off(onConnectionStateChanged);
                    const errorMessage = `Connection closed while waiting for request ${networkMessage_1.displayNetworkMessageType(message.type)}: ${rpcId}`;
                    request.reject(new connections_1.NetworkError(errorMessage));
                }
            };
            const clearDisconnectHandler = () => {
                this.requests.get(rpcId)?.peer.onStateChanged.off(onConnectionStateChanged);
            };
            const timeout = setTimeout(() => {
                const request = this.requests.get(rpcId);
                if (!request) {
                    throw new Error(`Timed out request ${rpcId} not found`);
                }
                const errorMessage = `Closing connections to ${peer.displayName} because RPC message of type ${networkMessage_1.displayNetworkMessageType(message.type)} timed out after ${rpcNetworkMessage_1.RPC_TIMEOUT_MILLIS} ms in request: ${rpcId}.`;
                const error = new connections_1.RequestTimeoutError(rpcNetworkMessage_1.RPC_TIMEOUT_MILLIS, errorMessage);
                this.logger.debug(errorMessage);
                clearDisconnectHandler();
                peer.close(error);
                request.reject(error);
            }, rpcNetworkMessage_1.RPC_TIMEOUT_MILLIS);
            const request = {
                resolve: (message) => {
                    clearDisconnectHandler();
                    peer.pendingRPC--;
                    this.requests.delete(rpcId);
                    clearTimeout(timeout);
                    const endTime = utils_1.BenchUtils.end(request.startTime);
                    this.metrics.p2p_RpcResponseTimeMsByMessage.get(request.messageType)?.add(endTime);
                    this.metrics.p2p_RpcSuccessRateByMessage.get(request.messageType)?.add(1);
                    resolve(message);
                },
                reject: (reason) => {
                    clearDisconnectHandler();
                    peer.pendingRPC--;
                    this.requests.delete(rpcId);
                    clearTimeout(timeout);
                    this.metrics.p2p_RpcSuccessRateByMessage.get(request.messageType)?.add(0);
                    reject(reason);
                },
                peer: peer,
                messageType: message.type,
                startTime: utils_1.BenchUtils.start(),
            };
            peer.pendingRPC++;
            this.requests.set(rpcId, request);
            const connection = peer.send(message);
            if (!connection) {
                return request.reject(new Error(`${String(peer.state.identity)} did not send ${networkMessage_1.displayNetworkMessageType(message.type)} in state ${peer.state.type}`));
            }
            peer.onStateChanged.on(onConnectionStateChanged);
        });
    }
    async getBlockHeaders(peer, start, limit, skip = 0, reverse = false) {
        const begin = utils_1.BenchUtils.start();
        const message = new getBlockHeaders_1.GetBlockHeadersRequest(start, limit, skip, reverse);
        const response = await this.requestFrom(peer, message);
        if (!(response instanceof getBlockHeaders_1.GetBlockHeadersResponse)) {
            // TODO jspafford: disconnect peer, or handle it more properly
            throw new Error(`Invalid GetBlockHeadersResponse: ${networkMessage_1.displayNetworkMessageType(message.type)}`);
        }
        return { headers: response.headers, time: utils_1.BenchUtils.end(begin) };
    }
    async getBlocks(peer, start, limit) {
        const begin = utils_1.BenchUtils.start();
        const message = new getBlocks_1.GetBlocksRequest(start, limit);
        const response = await this.requestFrom(peer, message);
        if (!(response instanceof getBlocks_1.GetBlocksResponse)) {
            // TODO jspafford: disconnect peer, or handle it more properly
            throw new Error(`Invalid GetBlocksResponse: ${networkMessage_1.displayNetworkMessageType(message.type)}`);
        }
        const exceededSoftLimit = response.getSize() >= version_1.SOFT_MAX_MESSAGE_SIZE;
        const isMessageFull = exceededSoftLimit || response.blocks.length >= limit;
        return { blocks: response.blocks, time: utils_1.BenchUtils.end(begin), isMessageFull };
    }
    async handleMessage(peer, message) {
        if (!this.localPeer.enableSyncing) {
            peer.punish(peer_1.BAN_SCORE.MAX);
            return;
        }
        if (message instanceof rpcNetworkMessage_1.RpcNetworkMessage) {
            await this.handleRpcMessage(peer, message);
        }
        else if (message instanceof newBlockHashes_1.NewBlockHashesMessage) {
            await this.handleNewBlockHashesMessage(peer, message);
        }
        else if (message instanceof newCompactBlock_1.NewCompactBlockMessage) {
            await this.onNewCompactBlock(peer, message.compactBlock);
        }
        else if (message instanceof newPooledTransactionHashes_1.NewPooledTransactionHashes) {
            this.handleNewPooledTransactionHashes(peer, message);
        }
        else if (message instanceof newTransactions_1.NewTransactionsMessage) {
            for (const transaction of message.transactions) {
                await this.onNewTransaction(peer, transaction);
            }
        }
        else {
            throw new Error(`Invalid message for handling in peer network: '${networkMessage_1.displayNetworkMessageType(message.type)}'`);
        }
    }
    /**
     * Handle an incoming RPC message. This may be an incoming request for some
     * data, or an incoming response to one of our requests.
     *
     * If it is a request, we pass it to the handler registered for it.
     * If a response, we resolve the promise waiting for it.
     *
     * The handler for a given request should either return a payload or throw
     * a CannotSatisfyRequest error
     */
    async handleRpcMessage(peer, rpcMessage) {
        const rpcId = rpcMessage.rpcId;
        if (rpcMessage.direction === rpcNetworkMessage_1.Direction.Request) {
            let responseMessage;
            try {
                if (rpcMessage instanceof getBlockHeaders_1.GetBlockHeadersRequest) {
                    responseMessage = await this.onGetBlockHeadersRequest(peer, rpcMessage);
                }
                else if (rpcMessage instanceof getBlocks_1.GetBlocksRequest) {
                    responseMessage = await this.onGetBlocksRequest(peer, rpcMessage);
                }
                else if (rpcMessage instanceof pooledTransactions_1.PooledTransactionsRequest) {
                    responseMessage = this.onPooledTransactionsRequest(rpcMessage, rpcId);
                }
                else if (rpcMessage instanceof getBlockTransactions_1.GetBlockTransactionsRequest) {
                    responseMessage = await this.onGetBlockTransactionsRequest(peer, rpcMessage);
                }
                else if (rpcMessage instanceof getCompactBlock_1.GetCompactBlockRequest) {
                    responseMessage = await this.onGetCompactBlockRequest(rpcMessage);
                }
                else {
                    throw new Error(`Invalid rpc message type: '${rpcMessage.type}'`);
                }
            }
            catch (error) {
                const asError = error;
                if (!(asError.name && asError.name === 'CannotSatisfyRequestError')) {
                    this.logger.error(`Unexpected error in ${networkMessage_1.displayNetworkMessageType(rpcMessage.type)} handler: ${String(error)}`);
                }
                responseMessage = new cannotSatisfyRequest_1.CannotSatisfyRequest(rpcId);
            }
            const sent = peer.send(responseMessage);
            if (sent &&
                responseMessage instanceof pooledTransactions_1.PooledTransactionsResponse &&
                peer.state.identity) {
                for (const transaction of responseMessage.transactions) {
                    const hash = transaction.hash();
                    this.markKnowsTransaction(hash, peer.state.identity);
                }
            }
        }
        else {
            const request = this.requests.get(rpcId);
            if (request) {
                request.resolve(rpcMessage);
            }
            else if (rpcMessage instanceof pooledTransactions_1.PooledTransactionsResponse) {
                for (const transaction of rpcMessage.transactions) {
                    await this.onNewTransaction(peer, transaction);
                }
            }
            else if (rpcMessage instanceof getBlockTransactions_1.GetBlockTransactionsResponse) {
                await this.onNewBlockTransactions(peer, rpcMessage);
            }
            else if (rpcMessage instanceof getCompactBlock_1.GetCompactBlockResponse) {
                await this.onNewCompactBlock(peer, rpcMessage.compactBlock);
            }
            else if (rpcMessage instanceof getBlocks_1.GetBlocksResponse) {
                // Should happen when block is requested directly by the block fetcher
                for (const block of rpcMessage.blocks) {
                    await this.handleRequestedBlock(peer, block);
                }
            }
        }
    }
    async handleNewBlockHashesMessage(peer, message) {
        if (!this.shouldProcessNewBlocks()) {
            return;
        }
        for (const { hash, sequence } of message.blockHashInfos) {
            peer.knownBlockHashes.set(hash, peer_1.KnownBlockHashesValue.Received);
            if (peer.sequence === null || sequence > peer.sequence) {
                peer.sequence = sequence;
            }
            // Request blocks that can be fetched as compact blocks, and that we don't already have.
            // NOTE: It may be possible to start syncing from peers who send hashes with a sequence
            // greater than 1 ahead of our chain head, but consider also adding protection against
            // peers who send hashes that map to invalid blocks.
            if (sequence >= this.chain.head.sequence - MAX_GET_COMPACT_BLOCK_DEPTH &&
                !(await this.alreadyHaveBlock(hash))) {
                this.blockFetcher.receivedHash(hash, peer);
            }
        }
    }
    async onNewBlockTransactions(peer, message) {
        const block = this.blockFetcher.receivedBlockTransactions(message);
        if (!block) {
            return;
        }
        // if we don't have the previous block, start syncing
        const prevHeader = await this.chain.getHeader(block.header.previousBlockHash);
        if (prevHeader === null) {
            this.chain.addOrphan(block.header);
            this.blockFetcher.removeBlock(block.header.hash);
            this.node.syncer.startSync(peer);
            return;
        }
        await this.onNewFullBlock(peer, block, prevHeader);
    }
    async onNewCompactBlock(peer, compactBlock) {
        if (!this.shouldProcessNewBlocks()) {
            return;
        }
        // mark the block as received in the block fetcher and decide whether to continue
        // to validate this compact block or not
        const shouldProcess = this.blockFetcher.receivedCompactBlock(compactBlock, peer);
        if (!shouldProcess) {
            return;
        }
        // verify the header
        const header = compactBlock.header;
        if (header.sequence === primitives_1.GENESIS_BLOCK_SEQUENCE) {
            this.chain.addInvalid(header.hash, consensus_1.VerificationResultReason.GOSSIPED_GENESIS_BLOCK);
            this.blockFetcher.removeBlock(header.hash);
            return;
        }
        const verifyHeaderResult = this.chain.verifier.verifyBlockHeader(header);
        if (!verifyHeaderResult.valid) {
            this.chain.addInvalid(header.hash, verifyHeaderResult.reason ?? consensus_1.VerificationResultReason.ERROR);
            this.blockFetcher.removeBlock(header.hash);
            return;
        }
        if (await this.alreadyHaveBlock(header)) {
            this.blockFetcher.removeBlock(header.hash);
            return;
        }
        // set values on the peer to indicate the peer has the block
        if (peer.sequence === null || header.sequence > peer.sequence) {
            peer.sequence = header.sequence;
        }
        // this might overwrite the existing value if we've already sent the
        // block to the peer, but the value isn't important
        peer.knownBlockHashes.set(header.hash, peer_1.KnownBlockHashesValue.Received);
        this.onBlockGossipReceived.emit(header);
        // if we don't have the previous block, start syncing
        const prevHeader = await this.chain.getHeader(header.previousBlockHash);
        if (prevHeader === null) {
            this.chain.addOrphan(header);
            this.blockFetcher.removeBlock(header.hash);
            this.node.syncer.startSync(peer);
            return;
        }
        // since we have the previous block, do contextual verification
        const { valid, reason } = this.chain.verifier.verifyBlockHeaderContextual(header, prevHeader);
        if (!valid) {
            this.chain.addInvalid(header.hash, reason ?? consensus_1.VerificationResultReason.ERROR);
            this.blockFetcher.removeBlock(header.hash);
            return;
        }
        // check if we're missing transactions
        const result = utils_1.CompactBlockUtils.assembleTransactionsFromMempool(this.node.memPool, compactBlock);
        if (!result.ok) {
            peer.punish(peer_1.BAN_SCORE.MAX);
            this.blockFetcher.requestFullBlock(header.hash);
            return;
        }
        const { missingTransactions, partialTransactions } = result;
        // log telemetry on how many transactions we already had in our the mempool
        // or on the compact block's transactions field
        this.telemetry.submitCompactBlockAssembled(header, missingTransactions.length, compactBlock.transactionHashes.length - missingTransactions.length);
        if (result.missingTransactions.length > 0) {
            this.blockFetcher.requestBlockTransactions(peer, header, partialTransactions, missingTransactions);
            return;
        }
        const fullTransactions = [];
        for (const partial of partialTransactions) {
            partial.type === 'FULL' && fullTransactions.push(partial.value);
        }
        const fullBlock = new block_1.Block(compactBlock.header, fullTransactions);
        await this.onNewFullBlock(peer, fullBlock, prevHeader);
    }
    handleNewPooledTransactionHashes(peer, message) {
        if (!this.shouldProcessTransactions()) {
            return;
        }
        for (const hash of message.hashes) {
            peer.state.identity && this.markKnowsTransaction(hash, peer.state.identity);
            // If the transaction is already on chain we will get it through a block
            if (this.recentlyAddedToChain.has(hash)) {
                continue;
            }
            // Recently evicted means the transaction is relatively low feeRate, the
            // mempool + wallet have already processed it and its already been gossiped
            // once. It could still be downloaded and forwarded to peers who have joined
            // since that initial gossip, but since it is a low feeRate and
            // other peer mempools are likely at capacity too, just drop it
            if (this.node.memPool.recentlyEvicted(hash)) {
                continue;
            }
            // If the transaction is already in the mempool we don't need to request
            // the full transaction. Just broadcast it
            const transaction = this.node.memPool.get(hash);
            if (transaction) {
                this.broadcastTransaction(transaction);
            }
            else {
                this.transactionFetcher.hashReceived(hash, peer);
            }
        }
    }
    updateIsReady() {
        const prevIsReady = this._isReady;
        this._isReady = this.started && this.peerManager.getConnectedPeers().length >= this.minPeers;
        if (this._isReady !== prevIsReady) {
            this.onIsReadyChanged.emit(this._isReady);
        }
    }
    async onGetBlockHeadersRequest(peer, request) {
        const rpcId = request.rpcId;
        if (request.limit === 0) {
            peer.punish(peer_1.BAN_SCORE.LOW, `Peer sent GetBlockHeaders with limit of ${request.limit}`);
            return new getBlockHeaders_1.GetBlockHeadersResponse([], rpcId);
        }
        if (request.limit > version_1.MAX_REQUESTED_HEADERS) {
            peer.punish(peer_1.BAN_SCORE.MAX, `Peer sent GetBlockHeaders with limit of ${request.limit}`);
            const error = new connections_1.CannotSatisfyRequestError(`Requested more than ${version_1.MAX_REQUESTED_HEADERS}`);
            throw error;
        }
        const start = request.start;
        const limit = request.limit;
        const skip = request.skip;
        const reverse = request.reverse;
        const from = await utils_1.BlockchainUtils.blockHeaderBySequenceOrHash(this.chain, start);
        if (!from) {
            return new getBlockHeaders_1.GetBlockHeadersResponse([], rpcId);
        }
        const headers = [];
        let skipCounter = skip;
        // Limit the total number of lookups to avoid excessive disk usage
        let remainingLookups = version_1.MAX_HEADER_LOOKUPS;
        // If `reverse` is true, we iterate in descending order, using `start` as the
        // highest sequence.  Otherwise, we iterate in ascending order, using
        // `start` as the lowest sequence.
        const iterationFunction = reverse
            ? (from) => this.chain.iterateFrom(from)
            : (from) => this.chain.iterateTo(from);
        for await (const header of iterationFunction(from)) {
            if (remainingLookups === 0) {
                break;
            }
            remainingLookups -= 1;
            if (skip) {
                if (skipCounter < skip) {
                    skipCounter += 1;
                    continue;
                }
                else if (skipCounter === skip) {
                    skipCounter = 0;
                }
            }
            headers.push(header);
            if (headers.length === limit) {
                break;
            }
        }
        return new getBlockHeaders_1.GetBlockHeadersResponse(headers, rpcId);
    }
    async onGetBlocksRequest(peer, request) {
        const rpcId = request.rpcId;
        if (request.limit === 0) {
            peer.punish(peer_1.BAN_SCORE.LOW, `Peer sent GetBlocks with limit of ${request.limit}`);
            return new getBlocks_1.GetBlocksResponse([], rpcId);
        }
        const start = request.start;
        const limit = request.limit;
        const from = await utils_1.BlockchainUtils.blockHeaderBySequenceOrHash(this.chain, start);
        if (!from) {
            return new getBlocks_1.GetBlocksResponse([], rpcId);
        }
        let remainingLookups = version_1.MAX_BLOCK_LOOKUPS;
        let totalSize = 0;
        const blocks = [];
        for await (const hash of this.chain.iterateToHashes(from)) {
            remainingLookups -= 1;
            const block = await this.chain.getBlock(hash);
            assert_1.Assert.isNotNull(block);
            totalSize += serializers_1.getBlockSize(block);
            blocks.push(block);
            if (blocks.length === limit ||
                totalSize >= version_1.SOFT_MAX_MESSAGE_SIZE ||
                remainingLookups === 0) {
                break;
            }
        }
        return new getBlocks_1.GetBlocksResponse(blocks, rpcId);
    }
    onPooledTransactionsRequest(message, rpcId) {
        const transactions = [];
        for (const hash of message.transactionHashes) {
            const transaction = this.node.memPool.get(hash);
            if (transaction) {
                transactions.push(transaction);
            }
        }
        return new pooledTransactions_1.PooledTransactionsResponse(transactions, rpcId);
    }
    async onGetBlockTransactionsRequest(peer, message) {
        let block = this.blockFetcher.getFullBlock(message.blockHash);
        if (block === null) {
            block = await this.chain.blockchainDb.db.withTransaction(null, async (tx) => {
                const header = await this.chain.getHeader(message.blockHash, tx);
                if (header === null) {
                    throw new connections_1.CannotSatisfyRequestError(`Peer requested transactions for block ${message.blockHash.toString('hex')} that isn't in the database`);
                }
                if (header.sequence < this.chain.head.sequence - MAX_GET_BLOCK_TRANSACTIONS_DEPTH) {
                    throw new connections_1.CannotSatisfyRequestError(`Peer requested transactions for block ${message.blockHash.toString('hex')} with sequence ${header.sequence} while chain head is at sequence ${this.chain.head.sequence}`);
                }
                const block = await this.chain.getBlock(header, tx);
                assert_1.Assert.isNotNull(block, 'Database should contain transactions if it contains block header');
                return block;
            });
        }
        if (message.transactionIndexes.length > block.transactions.length) {
            const errorMessage = `Requested ${message.transactionIndexes.length} transactions for block ${block.header.hash.toString('hex')} that contains ${block.transactions.length} transactions`;
            throw new connections_1.CannotSatisfyRequestError(errorMessage);
        }
        const transactions = [];
        let currentIndex = 0;
        for (const transactionIndex of message.transactionIndexes) {
            if (transactionIndex < 0) {
                const errorMessage = `Requested negative transaction index`;
                throw new connections_1.CannotSatisfyRequestError(errorMessage);
            }
            currentIndex += transactionIndex;
            if (currentIndex >= block.transactions.length) {
                const errorMessage = `Requested transaction index past the end of the block's transactions`;
                throw new connections_1.CannotSatisfyRequestError(errorMessage);
            }
            transactions.push(block.transactions[currentIndex]);
            currentIndex++;
        }
        return new getBlockTransactions_1.GetBlockTransactionsResponse(message.blockHash, transactions, message.rpcId);
    }
    async onGetCompactBlockRequest(message) {
        const block = await this.chain.blockchainDb.db.withTransaction(null, async (tx) => {
            const header = await this.chain.getHeader(message.blockHash, tx);
            if (header === null) {
                throw new connections_1.CannotSatisfyRequestError(`Peer requested compact block for block ${message.blockHash.toString('hex')} that isn't in the database`);
            }
            if (header.sequence < this.chain.head.sequence - MAX_GET_COMPACT_BLOCK_DEPTH) {
                throw new connections_1.CannotSatisfyRequestError(`Peer requested compact block for ${message.blockHash.toString('hex')} with sequence ${header.sequence} while chain head is at sequence ${this.chain.head.sequence}`);
            }
            const block = await this.chain.getBlock(header, tx);
            assert_1.Assert.isNotNull(block, 'Database should contain transactions if it contains block header');
            return block;
        });
        return new getCompactBlock_1.GetCompactBlockResponse(block.toCompactBlock(), message.rpcId);
    }
    async handleRequestedBlock(peer, block) {
        if (!this.shouldProcessNewBlocks()) {
            return;
        }
        if (await this.alreadyHaveBlock(block.header)) {
            return;
        }
        peer.knownBlockHashes.set(block.header.hash, peer_1.KnownBlockHashesValue.Received);
        // verify the block header
        if (block.header.sequence === primitives_1.GENESIS_BLOCK_SEQUENCE) {
            this.chain.addInvalid(block.header.hash, consensus_1.VerificationResultReason.GOSSIPED_GENESIS_BLOCK);
            this.blockFetcher.removeBlock(block.header.hash);
            return;
        }
        const verifyBlockHeaderResult = this.chain.verifier.verifyBlockHeader(block.header);
        if (!verifyBlockHeaderResult.valid) {
            this.chain.addInvalid(block.header.hash, verifyBlockHeaderResult.reason ?? consensus_1.VerificationResultReason.ERROR);
            this.blockFetcher.removeBlock(block.header.hash);
            return;
        }
        if (!peer.sequence || block.header.sequence > peer.sequence) {
            peer.sequence = block.header.sequence;
        }
        this.onBlockGossipReceived.emit(block.header);
        // if we don't have the previous block, start syncing
        const prevHeader = await this.chain.getHeader(block.header.previousBlockHash);
        if (prevHeader === null) {
            this.chain.addOrphan(block.header);
            this.blockFetcher.removeBlock(block.header.hash);
            this.node.syncer.startSync(peer);
            return;
        }
        await this.onNewFullBlock(peer, block, prevHeader);
    }
    async onNewFullBlock(peer, block, prevHeader) {
        if (!this.shouldProcessNewBlocks()) {
            return;
        }
        // Mark that we've assembled a full block in the block fetcher
        this.blockFetcher.receivedFullBlock(block);
        this.broadcastBlock(block);
        // log that we've validated the block enough to gossip it
        this.telemetry.submitNewBlockSeen(block, new Date());
        // verify the full block
        const verified = await this.chain.verifier.verifyBlockAdd(block, prevHeader);
        if (!verified.valid) {
            this.chain.addInvalid(block.header.hash, verified.reason ?? consensus_1.VerificationResultReason.ERROR);
            this.blockFetcher.removeBlock(block.header.hash);
            return;
        }
        // add the block to the chain
        const result = await this.node.syncer.addBlock(peer, block);
        // We should have checked if the block is an orphan or duplicate already, so we
        // don't have to handle those cases here. If there was a verification error, the
        // chain should have added the block to the invalid set.
        if (result.added) {
            this.broadcastBlockHash(block.header);
        }
    }
    shouldProcessNewBlocks() {
        // We drop blocks when we are still initially syncing as they
        // will become loose blocks and we can't verify them
        if (!this.chain.synced && this.node.syncer.loader) {
            return false;
        }
        return this.enableSyncing;
    }
    shouldProcessTransactions() {
        if (!this.enableSyncing) {
            return false;
        }
        // Ignore new transactions if the node is still syncing
        //
        // TODO(rohanjadvani): However, it's okay to accept transactions if you are
        // not synced and not syncing. We should update this logic after syncing
        // becomes more reliable
        if (!this.node.chain.synced) {
            return false;
        }
        // TODO: We may want to remove this so that transactions still propagate
        // even with a full worker pool
        if (this.node.workerPool.saturated) {
            return false;
        }
        return true;
    }
    async alreadyHaveBlock(headerOrHash) {
        const hash = Buffer.isBuffer(headerOrHash) ? headerOrHash : headerOrHash.hash;
        if (this.chain.isInvalid(headerOrHash)) {
            return true;
        }
        if (this.chain.orphans.has(hash)) {
            return true;
        }
        return await this.chain.hasBlock(hash);
    }
    async onNewTransaction(peer, transaction) {
        const received = new Date();
        const hash = transaction.hash();
        // Let the fetcher know that a transaction was received so it no longer queries for it
        this.transactionFetcher.receivedTransaction(hash);
        // Mark the peer as knowing about the transaction
        peer.state.identity && this.markKnowsTransaction(hash, peer.state.identity);
        if (!this.shouldProcessTransactions()) {
            this.transactionFetcher.removeTransaction(hash);
            return;
        }
        // TODO(daniel): This is used to quickly filter double spend transactions and save on
        // verification. Could be combined with double spend check in verifyNewTransaction
        if (this.recentlyAddedToChain.has(hash)) {
            this.transactionFetcher.removeTransaction(hash);
            return;
        }
        // If transaction is already in mempool that means it's been synced to the
        // wallet and the mempool so all that is left is to broadcast
        if (this.node.memPool.exists(hash)) {
            this.broadcastTransaction(transaction);
            this.transactionFetcher.removeTransaction(hash);
            return;
        }
        // Check that the transaction is valid
        const { valid, reason } = await this.chain.verifier.verifyNewTransaction(transaction);
        // Tell other systems like the wallet and block explorer syncer
        // about new transactions from the network
        this.onTransactionGossipReceived.emit(transaction, valid);
        if (!valid) {
            assert_1.Assert.isNotUndefined(reason);
            this.logger.debug(`Invalid transaction '${hash.toString('hex')}': ${reason}`);
            this.transactionFetcher.removeTransaction(hash);
            return;
        }
        const accepted = this.node.memPool.acceptTransaction(transaction);
        // At this point the only reasons a transaction is not accepted would be
        // overlapping nullifiers or it's expired. In both cases don't broadcast
        if (accepted) {
            this.onTransactionAccepted.emit(transaction, received);
            this.broadcastTransaction(transaction);
        }
        this.transactionFetcher.removeTransaction(hash);
    }
}
exports.PeerNetwork = PeerNetwork;
//# sourceMappingURL=peerNetwork.js.map